{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Question answering model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import pipeline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\r\n",
    "  \r\n",
    "class QA:\r\n",
    "    \r\n",
    "    def __init__(self):\r\n",
    "        self.pipeline = pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\")\r\n",
    "        \r\n",
    "    def answer_question(self, question, context):\r\n",
    "        return self.pipeline(question=question, context=context)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "qa = QA()\r\n",
    "question = 'tell me my age?'\r\n",
    "context = \"My name is alex and i'm not really fifteen hundred years old but rather five. I drive a scuba diver dog named patty to work every day. it's fast as hell let me tell you! and i lied about my age i am actually twenty\"\r\n",
    "\r\n",
    "qa.answer_question(question=question, context=context)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'score': 0.3329688310623169, 'start': 208, 'end': 214, 'answer': 'twenty'}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Text generation model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from transformers import pipeline\r\n",
    "class TextGenerator:\r\n",
    "    \r\n",
    "    def __init__(self):\r\n",
    "        self.pipeline = pipeline(\"text-generation\", model='distilgpt2')\r\n",
    "        \r\n",
    "    def generate_text(self, context, min_length=50, max_length=500):\r\n",
    "        \" Generates text from the context\"\r\n",
    "        return self.pipeline(context, min_length=min_length, max_length=max_length)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "textpipe = pipeline(\"text-generation\", model='distilgpt2')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 762/762 [00:00<00:00, 350kB/s]\n",
      "Downloading: 100%|██████████| 353M/353M [00:30<00:00, 11.4MB/s]\n",
      "Downloading: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.65MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 1.10MB/s]\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 1.66MB/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "tg = TextGenerator()\r\n",
    "tg.generate_text('hi my name', max_length=50)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'generated_text': \"hi my name is Hui! I have to admit that I'm not a bad person to wear, be a lot more mature in some respects, with a lot of other things in my heart, including my own personality, my family and other things\"}]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment analysis model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "\r\n",
    "class SentimentAnalyser:\r\n",
    "    \r\n",
    "    def __init__(self):\r\n",
    "        self.pipeline = pipeline(\"sentiment-analysis\")\r\n",
    "                                 \r\n",
    "    def analyse_text(self, text):\r\n",
    "        return self.pipeline(text)\r\n",
    "                                 "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "sa = SentimentAnalyser()\r\n",
    "sa.analyse_text('I hate you')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9991129040718079}]"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "dir(sa.pipeline.model)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_convert_head_mask_to_5d',\n",
       " '_expand_inputs_for_generation',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_backward_hooks',\n",
       " '_get_decoder_start_token_id',\n",
       " '_get_logits_processor',\n",
       " '_get_logits_warper',\n",
       " '_get_name',\n",
       " '_get_pad_token_id',\n",
       " '_get_resized_embeddings',\n",
       " '_get_resized_lm_head',\n",
       " '_get_stopping_criteria',\n",
       " '_hook_rss_memory_post_forward',\n",
       " '_hook_rss_memory_pre_forward',\n",
       " '_init_weights',\n",
       " '_is_full_backward_hook',\n",
       " '_keys_to_ignore_on_load_missing',\n",
       " '_keys_to_ignore_on_load_unexpected',\n",
       " '_keys_to_ignore_on_save',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_into_model',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_prepare_attention_mask_for_generation',\n",
       " '_prepare_decoder_input_ids_for_generation',\n",
       " '_prepare_encoder_decoder_kwargs_for_generation',\n",
       " '_prepare_input_ids_for_generation',\n",
       " '_push_to_hub',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_reorder_cache',\n",
       " '_replicate_for_data_parallel',\n",
       " '_resize_token_embeddings',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_tie_encoder_decoder_weights',\n",
       " '_tie_or_clone_weights',\n",
       " '_update_model_kwargs_for_generation',\n",
       " '_version',\n",
       " 'add_memory_hooks',\n",
       " 'add_module',\n",
       " 'adjust_logits_during_generation',\n",
       " 'apply',\n",
       " 'base_model',\n",
       " 'base_model_prefix',\n",
       " 'beam_sample',\n",
       " 'beam_search',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'classifier',\n",
       " 'config',\n",
       " 'config_class',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'device',\n",
       " 'distilbert',\n",
       " 'double',\n",
       " 'dropout',\n",
       " 'dtype',\n",
       " 'dummy_inputs',\n",
       " 'dump_patches',\n",
       " 'estimate_tokens',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'floating_point_ops',\n",
       " 'forward',\n",
       " 'from_pretrained',\n",
       " 'generate',\n",
       " 'get_buffer',\n",
       " 'get_extended_attention_mask',\n",
       " 'get_head_mask',\n",
       " 'get_input_embeddings',\n",
       " 'get_output_embeddings',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'greedy_search',\n",
       " 'group_beam_search',\n",
       " 'half',\n",
       " 'init_weights',\n",
       " 'invert_attention_mask',\n",
       " 'is_parallelizable',\n",
       " 'load_state_dict',\n",
       " 'load_tf_weights',\n",
       " 'modules',\n",
       " 'name_or_path',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'num_labels',\n",
       " 'num_parameters',\n",
       " 'parameters',\n",
       " 'pre_classifier',\n",
       " 'prepare_inputs_for_generation',\n",
       " 'prune_heads',\n",
       " 'push_to_hub',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'reset_memory_hooks_state',\n",
       " 'resize_token_embeddings',\n",
       " 'retrieve_modules_from_names',\n",
       " 'sample',\n",
       " 'save_pretrained',\n",
       " 'set_input_embeddings',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'tie_weights',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Efficientnet\n",
    "\n",
    "This doesn't work. The model has no classification head"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "!pip install efficientnet_pytorch"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting efficientnet_pytorch\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages (from efficientnet_pytorch) (1.9.0+cu111)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\alexanderhagelborn\\miniconda3\\envs\\testenv\\lib\\site-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n",
      "Building wheels for collected packages: efficientnet-pytorch\n",
      "  Building wheel for efficientnet-pytorch (setup.py): started\n",
      "  Building wheel for efficientnet-pytorch (setup.py): finished with status 'done'\n",
      "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=200aee346758d15f1cbd0fdbaa25823c05e0979dbc579e7e1e295ba11ac10e60\n",
      "  Stored in directory: c:\\users\\alexanderhagelborn\\appdata\\local\\pip\\cache\\wheels\\84\\b9\\90\\25a0195cf95fb5533db96f1c77ea3f296b7cc86ae8ae48e3dc\n",
      "Successfully built efficientnet-pytorch\n",
      "Installing collected packages: efficientnet-pytorch\n",
      "Successfully installed efficientnet-pytorch-0.7.1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "from efficientnet_pytorch import EfficientNet\r\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to C:\\Users\\AlexanderHagelborn/.cache\\torch\\hub\\checkpoints\\efficientnet-b0-355c32eb.pth\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0.00/20.4M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63fdc94c1b154d82a3f7e0f33d7c5f9a",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "import torch\r\n",
    "inputs = torch.rand(1, 3, 224, 224)\r\n",
    "model.eval()\r\n",
    "outputs = model(inputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CLIP for image classification\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\r\n",
    "from PIL import Image\r\n",
    "\r\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\r\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 3.98k/3.98k [00:00<00:00, 3.98MB/s]\n",
      "Downloading: 100%|██████████| 605M/605M [00:53<00:00, 11.3MB/s]\n",
      "Downloading: 100%|██████████| 316/316 [00:00<00:00, 106kB/s]\n",
      "Downloading: 100%|██████████| 862k/862k [00:00<00:00, 1.37MB/s]\n",
      "Downloading: 100%|██████████| 525k/525k [00:00<00:00, 1.16MB/s]\n",
      "Downloading: 100%|██████████| 389/389 [00:00<00:00, 404kB/s]\n",
      "Downloading: 100%|██████████| 568/568 [00:00<00:00, 256kB/s]\n",
      "Downloading: 100%|██████████| 1.49M/1.49M [00:00<00:00, 1.87MB/s]\n",
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from PIL import Image\r\n",
    "import PIL\r\n",
    "\r\n",
    "class ImageClassifier:\r\n",
    "    \r\n",
    "    def __init__(self, labels = ['cat', 'dog', 'banana']):\r\n",
    "        self.labels = labels\r\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\r\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\r\n",
    "        \r\n",
    "    def classify(self, image: PIL.JpegImagePlugin.JpegImageFile):\r\n",
    "        inputs = processor(text=self.labels, images=image, return_tensors=\"pt\", padding=True)\r\n",
    "        outputs = model(**inputs)\r\n",
    "        logits_per_image = outputs.logits_per_image # this is the image-text similarity score\r\n",
    "        probs = logits_per_image.softmax(dim=1).detach().numpy()[0] # we can take the softmax to get the label probabilities\r\n",
    "        return self._yield_output(probs, self.labels)\r\n",
    "        \r\n",
    "    def _yield_output(self, probs, labels):\r\n",
    "        \"Returns a dict mapping from label to probability\"\r\n",
    "        result = {z[0]:z[1] for z in zip(labels, probs)}\r\n",
    "        return result\r\n",
    "        \r\n",
    "    def change_labels(self, new_labels: list):\r\n",
    "        \"Changes the class labels that the model uses\"\r\n",
    "        self.labels = new_labels\r\n",
    "        \r\n",
    "    def get_labels(self):\r\n",
    "        \"Returns the class labels the model uses at inference\"\r\n",
    "        return self.labels\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "clip = ImageClassifier(labels = ['cat', 'dog', 'fish'])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "image = Image.open(\"cat.jpg\")\r\n",
    "print(type(image))\r\n",
    "clip.classify(image)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'cat': 0.98979366, 'dog': 0.008394079, 'fish': 0.0018122782}"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('ec-ml-models2': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "5ec7335cfb73c3206946f991c2cd2bbec35668cc8d213faebd81e9bffc9c6d47"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}